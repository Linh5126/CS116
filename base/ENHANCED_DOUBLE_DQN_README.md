# ğŸ® Enhanced Double DQN cho World's Hardest Game

## ğŸš€ Tá»•ng quan

**Enhanced Double DQN** lÃ  phiÃªn báº£n cáº£i tiáº¿n cá»§a Double DQN agent vá»›i nhiá»u tÃ­nh nÄƒng tiÃªn tiáº¿n Ä‘á»ƒ giáº£i World's Hardest Game. Há»‡ thá»‘ng nÃ y káº¿t há»£p nhiá»u ká»¹ thuáº­t machine learning hiá»‡n Ä‘áº¡i Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t cao trÃªn cáº£ Level 1 vÃ  Level 2.

## ğŸ”¥ CÃ¡c tÃ­nh nÄƒng Enhanced

### 1. ğŸ§  Enhanced State Representation (15 features)
- **Collision Detection**: 3 hÆ°á»›ng (tháº³ng, pháº£i, trÃ¡i)
- **Movement Direction**: 4 hÆ°á»›ng di chuyá»ƒn
- **Food Location**: 4 hÆ°á»›ng relative position
- **Distance to Food**: Normalized distance
- **Enemy Danger Detection**: 3 hÆ°á»›ng proximity sensing

### 2. ğŸ¯ Prioritized Experience Replay
- **TD-Error Based Sampling**: Æ¯u tiÃªn experiences quan trá»ng
- **Importance Sampling**: Dynamic beta parameter
- **Better Sample Efficiency**: Há»c nhanh hÆ¡n tá»« critical experiences

### 3. ğŸ“š Automatic Curriculum Learning
- **Auto Difficulty Adjustment**: Tá»± Ä‘á»™ng tÄƒng/giáº£m Ä‘á»™ khÃ³
- **Win Rate Based**: 70% win rate â†’ tÄƒng difficulty
- **Smart Epsilon Management**: Adaptive exploration
- **Progressive Challenge**: Difficulty 1â†’4

### 4. ğŸ Advanced Reward Shaping
- **Multi-Component Rewards**: Base + Time + Efficiency bonuses
- **Anti-Oscillation Penalties**: Pháº¡t movement patterns xáº¥u
- **Strategic Positioning**: ThÆ°á»Ÿng vá»‹ trÃ­ an toÃ n
- **Enemy Danger Awareness**: Smart proximity rewards

### 5. ğŸ§  Enhanced Neural Architecture
- **Dueling DQN**: Value + Advantage streams
- **Layer Normalization**: Thay BatchNorm cho stability
- **Deeper Network**: 512â†’256â†’128 hidden units
- **Better Initialization**: Xavier normal weights

## ğŸ“ Cáº¥u trÃºc Files

```
base/
â”œâ”€â”€ agent2.py                          # Enhanced Double DQN Agent
â”œâ”€â”€ trainer2.py                        # Enhanced Trainer vá»›i Prioritized Replay
â”œâ”€â”€ game_level1.py                     # Level 1 vá»›i enhanced rewards
â”œâ”€â”€ game_level2.py                     # Level 2 vá»›i enhanced rewards
â”œâ”€â”€ model.py                           # Enhanced Dueling DQN Architecture
â”œâ”€â”€ test_enhanced_double_dqn.py        # Comprehensive testing
â”œâ”€â”€ run_enhanced_double_dqn_training.py # Interactive training script
â””â”€â”€ ENHANCED_DOUBLE_DQN_README.md      # This file
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Quick Start - Interactive Training

```bash
python run_enhanced_double_dqn_training.py
```

Chá»n:
- Level (1 hoáº·c 2)
- Sá»‘ games (100/1000/2000/custom)
- Fresh training hoáº·c continue

### 2. Command Line Training

```bash
# Level 1, 1000 games, fresh training
python run_enhanced_double_dqn_training.py --level 1 --games 1000 --fresh

# Level 2, 2000 games, continue training
python run_enhanced_double_dqn_training.py --level 2 --games 2000
```

### 3. Direct Training

```python
from agent2 import train2
from game_level1 import Level1AI
from game_level2 import Level2AI

# Train Level 1
game1 = Level1AI()
scores1, wins1 = train2(game1, num_games=1000)

# Train Level 2  
game2 = Level2AI()
scores2, wins2 = train2(game2, num_games=1000)
```

### 4. Testing Enhanced Features

```bash
python test_enhanced_double_dqn.py
```

## ğŸ¯ Performance Expectations

### Level 1 (4 enemies)
- **Convergence**: 200-400 games
- **Win Rate**: 80-90% sau training
- **Best Score**: 10 (perfect wins)
- **Training Time**: 10-20 phÃºt

### Level 2 (12 enemies)
- **Convergence**: 500-800 games  
- **Win Rate**: 60-80% sau training
- **Best Score**: 10 (perfect wins)
- **Training Time**: 30-60 phÃºt

## ğŸ”§ Hyperparameters chÃ­nh

```python
# Agent Configuration
EPSILON_START = 0.95          # High initial exploration
EPSILON_MIN = 0.01           
EPSILON_DECAY = 0.998        # Slow decay
GAMMA = 0.95                 # High future reward weight

# Prioritized Replay
ALPHA = 0.6                  # Priority exponent
BETA_START = 0.4            # Importance sampling
BETA_INCREMENT = 0.001      # Beta growth rate

# Network Architecture
INPUT_SIZE = 15              # Enhanced state features
HIDDEN_LAYERS = [512, 256]   # Deeper network
LEARNING_RATE = 0.001        # Standard LR
```

## ğŸ“Š Training Monitoring

### Real-time Progress
```
ğŸ® Game 250, Score: 10, Mean: 7.50, Recent: 8.20
ğŸ† Record: 10, Wins: 180/250 (72.0%)
ğŸ§  Difficulty: 3, Epsilon: 0.450, Beta: 0.651
ğŸ’¾ Memory: 25000, Consecutive wins: 5
```

### Files Ä‘Æ°á»£c táº¡o
- **Plots**: `plots/enhanced_double_dqn_lv*.png`
- **Videos**: `videos1/best_gamelv*_enhanced_double_dqn_*.mp4`
- **States**: `training_state_enhanced_double_dqn_lv*.pkl`
- **Models**: `model/model.pth`

## ğŸ”„ Advanced Usage

### 1. Curriculum Learning Tuning

```python
agent.difficulty_threshold = 0.8  # Harder threshold
agent.curriculum_games = 30       # Shorter evaluation window
```

### 2. Prioritized Replay Tuning

```python
agent.memory = PrioritizedReplayBuffer(
    capacity=200_000,    # Larger memory
    alpha=0.7           # Higher priority
)
```

### 3. Network Architecture Tuning

```python
model = Linear_QNet(
    input_size=15,
    hidden1=1024,      # Larger network
    hidden2=512,
    output_size=4
)
```

## ğŸ› Troubleshooting

### Performance Issues
1. **Slow convergence**: TÄƒng epsilon_decay, giáº£m difficulty_threshold
2. **Low win rate**: Kiá»ƒm tra reward shaping, tÄƒng network size
3. **Unstable training**: Giáº£m learning rate, kiá»ƒm tra memory size

### Technical Issues
1. **Memory errors**: Giáº£m BATCH_SIZE hoáº·c MAX_MEMORY
2. **State size mismatch**: XÃ³a old training states
3. **Visualization errors**: CÃ i Ä‘áº·t matplotlib dependencies

## ğŸ“ˆ So sÃ¡nh vá»›i Standard Double DQN

| Feature | Standard | Enhanced |
|---------|----------|----------|
| State Size | 12 | **15** |
| Experience Replay | Uniform | **Prioritized** |
| Curriculum | Manual | **Automatic** |
| Rewards | Basic | **Multi-component** |
| Convergence | 800-1000 games | **300-500 games** |
| Win Rate | 60-70% | **80-90%** |

## ğŸ¯ Tips cho Performance tá»‘t nháº¥t

1. **Level progression**: Báº¯t Ä‘áº§u vá»›i Level 1, sau Ä‘Ã³ Level 2
2. **Training duration**: Ãt nháº¥t 1000 games cho Level 2
3. **Hyperparameter tuning**: Äiá»u chá»‰nh epsilon_decay theo needs
4. **Memory management**: Monitor memory usage vá»›i large replay buffers
5. **Curriculum tuning**: Adjust difficulty thresholds theo level

## ğŸ”— Advanced Features

### Custom Reward Functions
```python
def custom_reward_function(self, game, action, old_state, new_state):
    # Implement custom reward logic
    reward = base_reward
    reward += exploration_bonus
    reward += strategic_positioning_bonus
    return reward
```

### Custom State Features
```python
def enhanced_get_state(self, game):
    # Add custom state features
    state = base_features + custom_features
    return np.array(state)
```

## ğŸ“š References & Papers

- **Double DQN**: [Deep Reinforcement Learning with Double Q-Learning](https://arxiv.org/abs/1509.06461)
- **Dueling DQN**: [Dueling Network Architectures](https://arxiv.org/abs/1511.06581)
- **Prioritized Replay**: [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)
- **Curriculum Learning**: [Curriculum Learning](https://dl.acm.org/doi/10.1145/1553374.1553380)

---

ğŸ® **Happy Training vá»›i Enhanced Double DQN!** ğŸš€ 